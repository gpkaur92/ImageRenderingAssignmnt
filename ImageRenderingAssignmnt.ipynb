{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Question2**: \n",
    "\n",
    "**Histogram equalization**:\n",
    "\n",
    "Histogram modeling techniques (e.g. histogram equalization) provide a sophisticated method for modifying the dynamic range and contrast of an image by altering that image such that its intensity histogram has a desired shape. Unlike contrast stretching, histogram modeling operators may employ non-linear and non-monotonic transfer functions to map between pixel intensity values in the input and output images. Histogram equalization employs a monotonic, non-linear mapping which re-assigns the intensity values of pixels in the input image such that the output image contains a uniform distribution of intensities (i.e. a flat histogram). This technique is used in image comparison processes (because it is effective in detail enhancement) and in the correction of non-linear effects introduced by, say, a digitizer or display system[1].\n",
    "\n",
    "**equalize_hist**:\n",
    "\n",
    "*Syntax*:\n",
    "\n",
    "skimage.exposure.equalize_hist(image, nbins=256, mask=None)[source]\n",
    "\n",
    "*Significance*:\n",
    "\n",
    "Return image after histogram equalization.\n",
    "\n",
    "*Parameters*:\t\n",
    "\n",
    "1> image : array-Image array.\n",
    "\n",
    "2> nbins : int, optional-Number of bins for image histogram. Note: this argument is ignored for integer images, for which each integer is its own bin.\n",
    "\n",
    "3> mask: ndarray of bools or 0s and 1s, optional-Array of same shape as image. Only points at which mask == True are used for the equalization, which is applied to the whole image.\n",
    "\n",
    "*Returns*:\t\n",
    "\n",
    "1> out : float array-Image array after histogram equalization.[2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Question3**:\n",
    "\n",
    "**Adaptive Histogram Equalization**:\n",
    "\n",
    "Adaptive histogram equalization (ahe) is a contrast enhancement method designed to be broadly applicable and having demonstrated effectiveness. However, slow speed and the overenhancement of noise it produces in relatively homogeneous regions are two problems.[3]\n",
    "\n",
    "**equalize_adapthist**:\n",
    "\n",
    "*Syntax*:\n",
    "\n",
    "skimage.exposure.equalize_adapthist(image, kernel_size=None, clip_limit=0.01, nbins=256)[source]\n",
    "\n",
    "*Significance*: Contrast Limited Adaptive Histogram Equalization (CLAHE).\n",
    "\n",
    "An algorithm for local contrast enhancement, that uses histograms computed over different tile regions of the image. Local details can therefore be enhanced even in regions that are darker or lighter than most of the image.\n",
    "\n",
    "*Parameters*:\t\n",
    "\n",
    "1> image : (M, N[, C]) ndarray-Input image.\n",
    "\n",
    "2> kernel_size: integer or list-like, optional-Defines the shape of contextual regions used in the algorithm. If iterable is passed, it must have the same number of elements as image.ndim (without color channel). If integer, it is broadcasted to each image dimension. By default, kernel_size is 1/8 of image height by 1/8 of its width.\n",
    "\n",
    "3> clip_limit : float, optional-Clipping limit, normalized between 0 and 1 (higher values give more contrast).\n",
    "\n",
    "4> nbins : int, optional-Number of gray bins for histogram (“data range”).\n",
    "\n",
    "*Returns*:\t\n",
    "\n",
    "1> out : (M, N[, C]) ndarray-Equalized image.[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question3**:\n",
    "\n",
    "*Histogram equalization* enhances the contrast of images by transforming the values in an intensity image so that the histogram of the output image approximately matches a specified histogram (uniform distribution by default).\n",
    "\n",
    "*Adaptive histogram equalization* performs contrast-limited adaptive histogram equalization. Unlike histogram equalization, it operates on small data regions (tiles) rather than the entire image. Each tile's contrast is enhanced so that the histogram of each output region approximately matches the specified histogram (uniform distribution by default). The contrast enhancement can be limited in order to avoid amplifying the noise which might be present in the image.\n",
    "\n",
    "This same effect we had observed in Imagerendering jupyter notebook.\n",
    "\n",
    "According me latter one (AHE) was more effective since we just have to view the image clearly instead of focusing on other artifacts and by changing clip limits we could see the difference between both techniques more clearly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**:\n",
    "\n",
    "[1] homepages.inf.ed.ac.uk/rbf/HIPR2/histeq.htm\n",
    "\n",
    "[2] scikit-image.org/docs/stable/api/skimage.exposure.html\n",
    "\n",
    "[3] *\"Adaptive histogram equalization and its variation\"*-Stephen.M.Pizer, E.Philip Amburn, John D.Austin, Robert Cromartie,Ari Geselowitz,Trey Greer, Bart ter Haar Romeny, Jzohn B. Zimmerman, Karel Zuiderveld.\n",
    "\n",
    "[4] *\"Adaptive image contrast enhancement using generalizations of histogram equalization\"*-J.A.Stark- Dept. of Engg.,University of Cambridge,UK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INVESTGATION QUESTION:**\n",
    "\n",
    "**Image registration:**\n",
    "\n",
    "The determination of a one-to-one mapping between the coordinates in one space and those in another,such that points in the two spaces that correspond to the same anatomical point are mapped to each other is called as registration. Now, moving to particularly image registration, it is establishing correspondence between features in sets of images,and using a transformation model to infer correspondence away from those features.\n",
    "\n",
    "Key Applications of this are found in:\n",
    "\n",
    "1> Change detection in Medical Imaging.\n",
    "\n",
    "2> Image fusion in Multi Modality Imaging[1].\n",
    "\n",
    "When we are done with the mapping of images through various image registration techniques. Then to analyze the output clearly we have to charatize the alignment of both images being mapped. This can be done by various methods. One of those techniques is entrophy.\n",
    "\n",
    "**Entrophy** *can be viewed in various ways: a measure of uncertainty in a random event (i.e., a measure of the “randomness” of a random variable), a measure of information obtained by observing a data source, and the dispersion, i.e.,scatterdness of a probability distribution.[2]*\n",
    "\n",
    "For a high dimensional discrete random variable X = (X1, . . . , Xd) ∈ Rd that has a probability mass function of p(x1, . . . , xd), the entropy formula (2.1) can be extended straightforwardly:\n",
    "H(X) = sum of p(x1+....+xd).log(1/p(x1,....,xd))\n",
    "Note that if Xi’s are independent and identically distributed with a p.m.f. q for all i,it is easy to see that H(X) = d · H(q). In information theory, an information source that produces such a random variable is usually called stationary and memoryless.\n",
    "Note that, in a general stationary source, i.e., if Xi’s are identically distributed with q; then H(X) ≤ d · H(q). That is the joint random variable cannot contain more information than the sum of the individual information entropies of the components.\n",
    "The upper bound is only achieved when all components are independent.[2]\n",
    "\n",
    "While entropy is the basic concept to build our approaches on, it is not the only information-theoretic measure that we use. We are also interested in relating several random variables and information theory contains many different measures for this purpose. These include conditional entropy, Kullback-Leibler divergence, mutual information and R´enyi entropy.\n",
    "\n",
    "The investigation of information-theoretic measures for image registration started in the 1990’s with Woods et al.’s seminal paper[3]. This also marks the beginning of the exploration of fast and reliable automatic multi-modal registration methods. The\n",
    "common trait of these approaches is that they rely on the whole image, particularly pixel/voxel intensity values, when determining the quality of alignment. This is contrary to landmark-based approaches that require the definition and computation of specific landmarks. These algorithms are constrained by the quality and speed of the landmark detection step.The basic idea that motivates the employment of information-theoretic measures for quantifying the quality of alignment is simple: Corresponding features extracted from the images should become statistically more dependent with better alignment[2].\n",
    "\n",
    "In an attempt to quantify the dispersion of the joint histogram it was proposed to employ the entropy of the joint\n",
    "histogram for determining alignment quality[3]. These studies were mainly based on the empirical observation that the joint distribution tends to be sharper with well-defined peaks at good alignment, which yields a small entropy. Experiments indicated that the approach was promising, yet no rigorous theoretical derivation was provided. Along with the joint entropy term, the mutual information formula includes marginal entropy terms. As argued by many authors,this makes mutual information a more suitable alignment measure where there’s limited scene overlap between images.In the following years, the basic idea of employing entropy-based measures for various multi-modal image registration applications yielded many successful algorithms[2].\n",
    "\n",
    "This has led to the success of automatic multi-modal registration algorithms that use information-theoretic alignment measures. However, it is important to note that these algorithms, by no means, provide a universal solution to image registration. They have two important drawbacks:\n",
    "1. Entropy-based alignment measures are typically computationally more expensive than simpler measures.\n",
    "2. In most implementations, entropy measures are computed based on the image intensity histograms. As discussed in many papers e.g.[4,5],this neglect of “spatial information” may affect alignment accuracy.\n",
    "\n",
    "**For using the same entrophy in image processing in python we use entrophy filter to create the variations[5].**\n",
    "\n",
    "\n",
    "\n",
    "**BIBLIOGRAPHY:**\n",
    "\n",
    "[1] *\"Image Registration in Medical Imaging\"*, Collin Studholme, Department of Radiology & Biomedical Imaging, University of California, San Francisco.\n",
    "\n",
    "[2] people.csail.mit.edu/msabuncu/thesis/Sabuncu_thesis.pdf\n",
    "\n",
    "[3] *“Rapid automated algorithm for aligning and reslicing pet images*”, R.Woods,S.Cherry,and J. Maziotta,*Journal of Comput Assist Tomogr.*, vol. 16, no. 4, pp. 622–633.\n",
    "\n",
    "[4] *“Mutual information based registration of medical images: A survey,”* IEEE Trans. on Medical Imaging, vol. 22, no. 8,    pp. 986–1004, 2003.\n",
    "\n",
    "[5] *“Spatial information in entropy-based image registration,”* M.Sabuncu and P.Ramadge in Biomedical Image Registration, LNCS 2717. Springer-Verlag, 2003.\n",
    "\n",
    "[6] scikit_image.org/docs/dev/auto_examples/filters/plot_entrophy.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
